{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215e84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import json\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "top_n_vals = [5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "cranfield = []\n",
    "with open(\"cranfield/cran_docs.json\", \"r\") as f:\n",
    "    cranfield = json.load(f)\n",
    "\n",
    "body_list = [data['body'] for data in cranfield]\n",
    "combined_text = \" \".join(body_list)\n",
    "\n",
    "# Load the model and tokenizer explicitly\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def get_keywords(text, top_n, model):\n",
    "    # Initialize KeyBERT with the loaded model\n",
    "    kw_model = KeyBERT(model=model)\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 1),\n",
    "        stop_words=\"english\",\n",
    "        use_mmr=True,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    # Save keywords to a CSV file\n",
    "    df = pd.DataFrame(keywords, columns=['Keyword', 'Score'])\n",
    "    df.to_csv(f\"topics/topics_top_{top_n}.csv\", index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Extract keywords in parallel\n",
    "    keyword_results = list(executor.map(get_keywords, [combined_text]*len(top_n_vals), top_n_vals, [model]*len(top_n_vals)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66f8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aaditmahajan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/aaditmahajan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='wiki_summary.log', filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_wikipedia_summary(keyword, max_attempts=3):\n",
    "    headers = {'User-Agent': 'NLP IR agent'}\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Search Wikipedia for the keyword\n",
    "    search_params = {\n",
    "        'action': 'query',\n",
    "        'list': 'search',\n",
    "        'srsearch': keyword,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "    search_data = search_response.json()\n",
    "\n",
    "    search_hits = search_data.get('query', {}).get('search', [])\n",
    "\n",
    "    # Try multiple hits (up to max_attempts)\n",
    "    for idx, hit in enumerate(search_hits[:max_attempts]):\n",
    "        title = hit['title']\n",
    "        \n",
    "        # Get the summary\n",
    "        summary_params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'extracts|categories',\n",
    "            'exintro': True,\n",
    "            'explaintext': True,\n",
    "            'titles': title,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        summary_response = requests.get(search_url, params=summary_params, headers=headers)\n",
    "        summary_data = summary_response.json()\n",
    "\n",
    "        pages = summary_data.get('query', {}).get('pages', {})\n",
    "        if pages:\n",
    "            page = next(iter(pages.values()))\n",
    "            summary = page.get('extract')\n",
    "            categories = [cat['title'] for cat in page.get('categories', [])] if 'categories' in page else []\n",
    "\n",
    "            # Check for disambiguation indicators\n",
    "            if summary:\n",
    "                is_disambig = (\"disambiguation pages\" in \" \".join(categories).lower() or\n",
    "                               \"may refer to\" in summary.lower() or\n",
    "                               \"may stand for\" in summary.lower())\n",
    "                \n",
    "                if not is_disambig:\n",
    "                    # Good summary found\n",
    "                    logging.info(f\"Found summary for '{keyword}'\")  # Print first 100 characters\n",
    "                    return summary\n",
    "                else:\n",
    "                    logging.info(f\"Disambiguation detected for '{title}', trying next result...\")\n",
    "\n",
    "    # Fallback to dictionary definition\n",
    "    synsets = wordnet.synsets(keyword)\n",
    "    if synsets:\n",
    "        return synsets[0].definition()\n",
    "\n",
    "    return None\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in top_n_vals:\n",
    "\n",
    "    df = pd.read_csv(f\"topics/topics_top_{i}.csv\")\n",
    "    df['Keyword'] = df['Keyword'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "    df.to_csv(f\"topics/topics_top_{i}_lem.csv\", index=False)\n",
    "\n",
    "    # Get Wikipedia summaries in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Extract Wikipedia summaries in parallel\n",
    "        wiki_results = list(executor.map(get_wikipedia_summary, df['Keyword']))\n",
    "\n",
    "    # Save Wikipedia summaries to a json\n",
    "    wiki_summaries = {df['Keyword'][i]: wiki_results[i] \n",
    "                      for i in range(len(wiki_results))\n",
    "                      if wiki_results[i] is not None}\n",
    "    with open(f\"topics/topics_top_{i}_lem_wiki.json\", \"w\") as f:\n",
    "        json.dump(wiki_summaries, f, indent=4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b818c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESA Vectors generated successfully.\n",
      "[0.044378063000222345, 0.044378063000222345, 0.044378063000222345, 0.04755062787844759, 0.0, 0.019375754127319955, 0.1466344915846601, 0.023407483290908786, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11793889753811097, 0.03967212815146216, 0.0, 0.05674916589948491, 0.023875933906285948, 0.0, 0.03044256118476253, 0.01785744351093431, 0.08401778914142174, 0.05942736317751768, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.020608076026420286, 0.0, 0.0, 0.0, 0.11793889753811097, 0.0, 0.0, 0.07904616146568252, 0.11865105196729897, 0.0, 0.0, 0.061367357100295346, 0.0, 0.0, 0.0, 0.12229784775631222, 0.013108107746545953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009983066887084828, 0.00773732684617084, 0.0, 0.0, 0.0, 0.0, 0.023465704878583304, 0.01946214875561731, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.029595535270625764, 0.020687972263834998, 0.0, 0.0, 0.0, 0.0, 0.0073753404852729375, 0.06504833299603477, 0.0, 0.020583992794123842, 0.008097141700602583, 0.0, 0.1441566580575947, 0.0, 0.0, 0.0, 0.01284028587802133, 0.0, 0.0133544144940423, 0.0609603291055807, 0.0, 0.023407483290908786, 0.008024003947033814, 0.0, 0.0, 0.0, 0.0, 0.07166912133517925, 0.0, 0.015861433231641336, 0.02406161427350434, 0.0, 0.0, 0.0, 0.0, 0.01102914419816985, 0.030581008260640516, 0.09956067659526652, 0.030081014367714563, 0.019300625991261193, 0.12424920736076166, 0.025179579685634484, 0.0, 0.0, 0.0, 0.023260604541228358, 0.0, 0.0, 0.05220628243730468, 0.03299813083085717, 0.0, 0.0, 0.03220083684182669, 0.0914342052506023, 0.0, 0.0038845186164226243, 0.006527095466108088, 0.0, 0.0, 0.0, 0.0, 0.034655528917802776, 0.0, 0.054997307869418276, 0.06184951239499343, 0.0, 0.05140881148184019, 0.0, 0.0, 0.0, 0.015641954909516857, 0.0, 0.010685682685699121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10998800510170455, 0.0, 0.018267070526691243, 0.006910230587335592, 0.03980883083524835, 0.029668462087991156, 0.0, 0.0, 0.0, 0.01831280088421126, 0.0, 0.0, 0.0, 0.0815170029307339, 0.0, 0.0, 0.04100124101488992, 0.0, 0.0, 0.0, 0.03306928012601905, 0.0, 0.0, 0.006024602436416723, 0.0, 0.05238652971714572, 0.0, 0.0, 0.0, 0.04791104043592507, 0.0, 0.0, 0.023345795881499022, 0.0, 0.0, 0.03248240911876413, 0.0, 0.0, 0.0, 0.0, 0.008989986992316656, 0.0, 0.0, 0.0, 0.0, 0.1737199152060046, 0.02744946018679546, 0.0, 0.05959595431423452, 0.0, 0.021915741593257966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015612884760246576, 0.018255075389989644, 0.0, 0.0, 0.0, 0.12823563483654163, 0.0, 0.009980369144997635, 0.0, 0.0, 0.07692538366621843, 0.0, 0.04124275831513416, 0.0, 0.008752241571780778, 0.0, 0.0, 0.0, 0.0, 0.015188944781593924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006976735523109226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009080369017716456, 0.05148003303493345, 0.025328528975798046, 0.0, 0.0, 0.0, 0.040091638938993666, 0.0, 0.0, 0.018381084660633443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023715406299505783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03823563319191696, 0.0, 0.0, 0.0, 0.0109406486455227, 0.0, 0.0, 0.12859254553752944, 0.0, 0.0, 0.0, 0.0, 0.07472767166984784, 0.0, 0.0, 0.03331525909891101, 0.006146028348037746, 0.04983488531822379, 0.057102687505798305, 0.0, 0.0, 0.060949981410535786, 0.018041634017049907, 0.0, 0.0, 0.046267447263119446, 0.0, 0.0, 0.0, 0.02672985756028304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023277777062674188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14280688537116007, 0.0, 0.007596662450984362, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02224736966037406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03433270234567577, 0.0, 0.016535150660824564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008029521772939504, 0.03969900233419584, 0.07235329555179394, 0.0, 0.0, 0.01188953883559784, 0.00942887082923691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01680859855374721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09078616132416954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019605951023802293, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005861228620537435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01621186879816264, 0.017240102214199707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01006255149075506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05629546184742243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012125244583716008, 0.0, 0.0, 0.02809097554156182, 0.0, 0.0, 0.0, 0.024588234582233023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.049388972474393444, 0.0, 0.0, 0.018199621983993615, 0.0, 0.0, 0.0, 0.0, 0.04354129942780399, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02006604087630178, 0.008161195391287067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23694436791903348, 0.0, 0.0, 0.0, 0.0, 0.006022395853333996, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009370384503929989, 0.017291942888350932, 0.0013300776090890887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010602415646384237, 0.009392517369027855, 0.0, 0.0, 0.0, 0.0, 0.016547165089905495, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01574405783159417, 0.00866424943431704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03301681173298372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017925196242313757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05336887653643845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034864815767767905, 0.0, 0.0, 0.010141985363473598, 0.0, 0.0, 0.0, 0.0, 0.01782750577521015, 0.0074078284440107795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0050439616190053604, 0.0, 0.0, 0.0, 0.0, 0.013474043322757489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007725729378354158, 0.0, 0.0, 0.0, 0.0, 0.010989449961191815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008337392191304565, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004335450151107271, 0.0, 0.0, 0.0, 0.0, 0.01384028019404804, 0.0, 0.0, 0.0, 0.01468602732522368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.047255003222232195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02117547311200099, 0.005679063925675592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008919875867484413, 0.13428881006908544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04338098071618474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01603589750264953, 0.0, 0.0, 0.006039866468673162, 0.014957820388898115, 0.0715549637970533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013441101233748993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.034939634924252984, 0.020743128473369336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010252269919217868, 0.0, 0.007878905897461827, 0.0, 0.0, 0.0, 0.0, 0.013560934990313026, 0.022433256472584834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016382489720287746, 0.0, 0.03443483676542751, 0.0, 0.018217239375484754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13959996014767942, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015780178480093818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01109656908341308, 0.0, 0.0, 0.0, 0.0068799080185266166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1156572687471763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0055494774439371185, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1263243631477565, 0.0, 0.0, 0.0, 0.008322834932328967, 0.030936538028108605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01926960629169796, 0.0, 0.0, 0.0, 0.005311987112946397, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007133055441239767, 0.014876712959079394, 0.0, 0.0, 0.0, 0.007356088401342659, 0.01104264896441123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01861902646616201, 0.0, 0.0067073839796592555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021771119626568042, 0.01174551995411352, 0.0, 0.021899242538564838, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00675150008987671, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006222166066169992, 0.006719892895438472, 0.014244485359262022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013318775838736014, 0.0, 0.0, 0.08888500714392414, 0.0, 0.008571960548866726, 0.0, 0.0, 0.0, 0.0, 0.02107158834278614, 0.0, 0.03375689288672665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019997089665450253]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wikipediaapi\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", filename=\"debug.log\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    '''\n",
    "    Preprocess the text by tokenizing, removing stop words, and lemmatizing.\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    '''\n",
    "\n",
    "    # Ensure nltk data is available when called by the Spark job later. \n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)  # Return as a string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Clean the text by removing non-alphanumeric characters and converting to lowercase.\n",
    "    (helper function)\n",
    "    '''\n",
    "    text = re.sub(r\"\\W+\", \" \", text.lower())\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_corpus(corpus_file=\"topics/topics_top_1000_lem_wiki.json\"):\n",
    "    '''\n",
    "    Load the corpus from a JSON file.\n",
    "    Args:\n",
    "        corpus_file (str): Path to the JSON file containing the corpus.\n",
    "    Returns:\n",
    "        dict: The loaded corpus as a dictionary.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(corpus_file, \"r\") as file:\n",
    "            corpus_dict = json.load(file)\n",
    "        logger.info(f\"Corpus successfully loaded from {corpus_file}.\")\n",
    "        return corpus_dict\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load corpus from {corpus_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def generate_esa_vectors(text, corpus_file=\"topics/topics_top_1000_lem_wiki.json\"):\n",
    "    '''\n",
    "    Generate ESA vectors for the given text using the lemmatized corpus.\n",
    "    Args:\n",
    "        text (str): The input text to generate ESA vectors for.\n",
    "    Returns:\n",
    "        list: The ESA vectors for the input text.\n",
    "    '''\n",
    "    \n",
    "    logger.info(\"Generating ESA vectors for artist.\")\n",
    "    \n",
    "    corpus = load_corpus(corpus_file)  # Load the corpus\n",
    "    if not corpus:\n",
    "        logger.error(\"Corpus is empty or could not be loaded.\")\n",
    "        return [], []\n",
    "\n",
    "    # preprocessing \n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = [preprocess_text(s) for s in sentences]\n",
    "    processed_corpus = list(corpus.values())\n",
    "    all_documents = processed_sentences + processed_corpus\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")          # Create a TF-IDF vectorizer\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_documents)      # Fit and transform the documents\n",
    "\n",
    "    esa_vectors = []\n",
    "    # Generate ESA vectors for each processed sentence\n",
    "    for i in range(len(processed_sentences)):\n",
    "        similarities = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix[len(processed_sentences):])\n",
    "        esa_vector = similarities.flatten()\n",
    "        esa_vectors.append(esa_vector)\n",
    "    \n",
    "    if esa_vectors:         # ESA vectors are generated\n",
    "        esa_vectors = np.mean(esa_vectors, axis=0)\n",
    "        return esa_vectors.tolist()\n",
    "    else:\n",
    "        logger.error(\"No ESA vectors generated.\")\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Main function to run the ESA vector generation.\n",
    "    '''\n",
    "    # Example text\n",
    "    text = \"what are the structural and aeroelastic problems associated with flight of high speed aircraft .\"\n",
    "    \n",
    "    # Generate ESA vectors for the example text\n",
    "    esa_vectors = generate_esa_vectors(text)\n",
    "    \n",
    "    if esa_vectors:\n",
    "        print(\"ESA Vectors generated successfully.\")\n",
    "        print(esa_vectors)\n",
    "    else:\n",
    "        print(\"Failed to generate ESA vectors.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea48d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
