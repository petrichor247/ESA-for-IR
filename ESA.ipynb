{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import json\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "top_n_vals = [5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "cranfield = []\n",
    "with open(\"cranfield/cran_docs.json\", \"r\") as f:\n",
    "    cranfield = json.load(f)\n",
    "\n",
    "body_list = [data['body'] for data in cranfield]\n",
    "combined_text = \" \".join(body_list)\n",
    "\n",
    "# Load the model and tokenizer explicitly\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def get_keywords(text, top_n, model):\n",
    "    # Initialize KeyBERT with the loaded model\n",
    "    kw_model = KeyBERT(model=model)\n",
    "\n",
    "    # Extract keywords\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 1),\n",
    "        stop_words=\"english\",\n",
    "        use_mmr=True,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    # Save keywords to a CSV file\n",
    "    df = pd.DataFrame(keywords, columns=['Keyword', 'Score'])\n",
    "    df.to_csv(f\"topics/topics_top_{top_n}.csv\", index=False)\n",
    "    return None\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Extract keywords in parallel\n",
    "    keyword_results = list(executor.map(get_keywords, [combined_text]*len(top_n_vals), top_n_vals, [model]*len(top_n_vals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import wordnet\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, filename='wiki_summary.log', filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def get_wikipedia_summary(keyword, max_attempts=3):\n",
    "    headers = {'User-Agent': 'NLP IR agent'}\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Search Wikipedia for the keyword\n",
    "    search_params = {\n",
    "        'action': 'query',\n",
    "        'list': 'search',\n",
    "        'srsearch': keyword,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "    search_data = search_response.json()\n",
    "\n",
    "    search_hits = search_data.get('query', {}).get('search', [])\n",
    "\n",
    "    # Try multiple hits (up to max_attempts)\n",
    "    for idx, hit in enumerate(search_hits[:max_attempts]):\n",
    "        title = hit['title']\n",
    "        \n",
    "        # Get the summary\n",
    "        summary_params = {\n",
    "            'action': 'query',\n",
    "            'prop': 'extracts|categories',\n",
    "            'exintro': True,\n",
    "            'explaintext': True,\n",
    "            'titles': title,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        summary_response = requests.get(search_url, params=summary_params, headers=headers)\n",
    "        summary_data = summary_response.json()\n",
    "\n",
    "        pages = summary_data.get('query', {}).get('pages', {})\n",
    "        if pages:\n",
    "            page = next(iter(pages.values()))\n",
    "            summary = page.get('extract')\n",
    "            categories = [cat['title'] for cat in page.get('categories', [])] if 'categories' in page else []\n",
    "\n",
    "            # Check for disambiguation indicators\n",
    "            if summary:\n",
    "                is_disambig = (\"disambiguation pages\" in \" \".join(categories).lower() or\n",
    "                               \"may refer to\" in summary.lower() or\n",
    "                               \"may stand for\" in summary.lower())\n",
    "                \n",
    "                if not is_disambig:\n",
    "                    # Good summary found\n",
    "                    logging.info(f\"Found summary for '{keyword}'\")  # Print first 100 characters\n",
    "                    return summary\n",
    "                else:\n",
    "                    logging.info(f\"Disambiguation detected for '{title}', trying next result...\")\n",
    "\n",
    "    # Fallback to dictionary definition\n",
    "    synsets = wordnet.synsets(keyword)\n",
    "    if synsets:\n",
    "        return synsets[0].definition()\n",
    "\n",
    "    return None\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i in top_n_vals:\n",
    "\n",
    "    df = pd.read_csv(f\"topics/topics_top_{i}.csv\")\n",
    "    df['Keyword'] = df['Keyword'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "    df.to_csv(f\"topics/topics_top_{i}_lem.csv\", index=False)\n",
    "\n",
    "    # Get Wikipedia summaries in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Extract Wikipedia summaries in parallel\n",
    "        wiki_results = list(executor.map(get_wikipedia_summary, df['Keyword']))\n",
    "\n",
    "    # Save Wikipedia summaries to a json\n",
    "    wiki_summaries = {df['Keyword'][i]: wiki_results[i] \n",
    "                      for i in range(len(wiki_results))\n",
    "                      if wiki_results[i] is not None}\n",
    "    with open(f\"topics/topics_top_{i}_lem_wiki.json\", \"w\") as f:\n",
    "        json.dump(wiki_summaries, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b818c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import wikipediaapi\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", filename=\"debug.log\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    '''\n",
    "    Preprocess the text by tokenizing, removing stop words, and lemmatizing.\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    '''\n",
    "\n",
    "    # Ensure nltk data is available when called by the Spark job later. \n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)  # Return as a string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Clean the text by removing non-alphanumeric characters and converting to lowercase.\n",
    "    (helper function)\n",
    "    '''\n",
    "    text = re.sub(r\"\\W+\", \" \", text.lower())\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_corpus(corpus_file=\"topics/topics_top_1000_lem_wiki.json\"):\n",
    "    '''\n",
    "    Load the corpus from a JSON file.\n",
    "    Args:\n",
    "        corpus_file (str): Path to the JSON file containing the corpus.\n",
    "    Returns:\n",
    "        dict: The loaded corpus as a dictionary.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(corpus_file, \"r\") as file:\n",
    "            corpus_dict = json.load(file)\n",
    "        logger.info(f\"Corpus successfully loaded from {corpus_file}.\")\n",
    "        return corpus_dict\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load corpus from {corpus_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def generate_esa_vectors(text, corpus_file=\"topics/topics_top_1000_lem_wiki.json\"):\n",
    "    '''\n",
    "    Generate ESA vectors for the given text using the lemmatized corpus.\n",
    "    Args:\n",
    "        text (str): The input text to generate ESA vectors for.\n",
    "    Returns:\n",
    "        list: The ESA vectors for the input text.\n",
    "    '''\n",
    "    \n",
    "    logger.info(\"Generating ESA vectors.\")\n",
    "    \n",
    "    corpus = load_corpus(corpus_file)  # Load the corpus\n",
    "    if not corpus:\n",
    "        logger.error(\"Corpus is empty or could not be loaded.\")\n",
    "        return [], []\n",
    "\n",
    "    # preprocessing \n",
    "    sentences = sent_tokenize(text)\n",
    "    processed_sentences = [preprocess_text(s) for s in sentences]\n",
    "    processed_corpus = list(corpus.values())\n",
    "    all_documents = processed_sentences + processed_corpus\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")          # Create a TF-IDF vectorizer\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_documents)      # Fit and transform the documents\n",
    "\n",
    "    esa_vectors = []\n",
    "    # Generate ESA vectors for each processed sentence\n",
    "    for i in range(len(processed_sentences)):\n",
    "        similarities = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix[len(processed_sentences):])\n",
    "        esa_vector = similarities.flatten()\n",
    "        esa_vectors.append(esa_vector)\n",
    "    \n",
    "    if esa_vectors:         # ESA vectors are generated\n",
    "        esa_vectors = np.mean(esa_vectors, axis=0)\n",
    "        return esa_vectors.tolist()\n",
    "    else:\n",
    "        logger.error(\"No ESA vectors generated.\")\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Main function to run the ESA vector generation.\n",
    "    '''\n",
    "    # Example text\n",
    "    text = \"what are the structural and aeroelastic problems associated with flight of high speed aircraft .\"\n",
    "    \n",
    "    # Generate ESA vectors for the example text\n",
    "    esa_vectors = generate_esa_vectors(text, corpus_file=\"topics/topics_top_1000_lem_wiki.json\")\n",
    "    \n",
    "    if esa_vectors:\n",
    "        print(\"ESA Vectors generated successfully.\")\n",
    "        print(esa_vectors)\n",
    "    else:\n",
    "        print(\"Failed to generate ESA vectors.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea48d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cranfield = []\n",
    "with open(\"cranfield/cran_docs.json\", \"r\") as f:\n",
    "    cranfield = json.load(f)\n",
    "    f.close()\n",
    "bodies = {data['id']:data['body'] for data in cranfield}\n",
    "    \n",
    "corpus_sizes = [5, 10, 50, 100, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for corpus_size in corpus_sizes:\n",
    "    \n",
    "    esa_dict = defaultdict()\n",
    "    \n",
    "    for doc_id, doc_body in bodies.items():\n",
    "        esa_dict[doc_id] = generate_esa_vectors(doc_body, corpus_file=f\"topics/topics_top_{corpus_size}_lem_wiki.json\")\n",
    "    \n",
    "    with open(f\"topics/topics_top_{corpus_size}_docs_esa.json\", \"w\") as f:\n",
    "        json.dump(esa_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
