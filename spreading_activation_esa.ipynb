{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2136f5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedb74a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESA Vectors generated successfully.\n",
      "[0.023516638436495416, 0.02910927056698486, 0.019357046230768234, 0.013727244662362548, 0.018578879595596953, 0.016586690265581117, 0.010569965263172734, 0.002499982658034518, 0.00993056104506468, 0.009717392531147648, 0.01268014463437572, 0.043011046740167064, 0.0, 0.019115975415987037, 0.03120154652873702, 0.05009823130537157, 0.0, 0.0, 0.01362944153877691, 0.04367344601795361, 0.008092892847158943, 0.034618200747395085, 0.00362483763172969, 0.00799323595876457, 0.0113970359499913, 0.0, 0.022781304258185593, 0.017648328296940255, 0.004918331340701366, 0.07331195159265422, 0.0024919299383025523, 0.018060178036941828, 0.004372008202407717, 0.006315726515891431, 0.007014909519073948, 0.0, 0.018578879595596953, 0.01815301548896441, 0.005559342246433639, 0.0, 0.017619692666780394, 0.016604223644225948, 0.0018325062091744716, 0.004834833026785754, 0.0071502707938283, 0.030731323265258466, 0.034697432186516115, 0.0, 0.01470262401576178, 0.0033356495970602285, 0.0, 0.028388578043298918, 0.0250831564684737, 0.006367868172293499, 0.02483665654959474, 0.026831716646046885, 0.002740668881097731, 0.002275049193086241, 0.02111990963967675, 0.009143038115268929, 0.0037570637567880183, 0.02676239591847755, 0.005415690631533646, 0.024731320646102603, 0.011344029427682617, 0.0, 0.01094295295882814, 0.0019036413320294007, 0.005607341339739893, 0.012006015033353613, 0.027030310158603098, 0.0, 0.004190479305390062, 0.004121728936308901, 0.0023556145879538985, 0.007682829608908714, 0.000568355310301344, 0.0, 0.013570087190599336, 0.012433814303382418, 0.002014160467587812, 0.006535561279950225, 0.0, 0.029990015789840112, 0.03740843021443006, 0.005807537368005001, 0.008899937632251848, 0.00964603508725223, 0.03707814092536625, 0.011432876882980413, 0.0, 0.02563867843484606, 0.007673103021847325]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import wikipediaapi\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", filename=\"debug.log\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    '''\n",
    "    Preprocess the text by tokenizing, removing stop words, and lemmatizing.\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    '''\n",
    "\n",
    "    # Ensure nltk data is available when called by the Spark job later. \n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.data.path.append(\"./nltk_data\")\n",
    "\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    tokens = [word for word in tokens if word.isalnum()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)  # Return as a string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    Clean the text by removing non-alphanumeric characters and converting to lowercase.\n",
    "    (helper function)\n",
    "    '''\n",
    "    text = re.sub(r\"\\W+\", \" \", text.lower())\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_corpus(corpus_file=\"topics/topics_top_1000_lem_wiki.json\"):\n",
    "    '''\n",
    "    Load the corpus from a JSON file.\n",
    "    Args:\n",
    "        corpus_file (str): Path to the JSON file containing the corpus.\n",
    "    Returns:\n",
    "        dict: The loaded corpus as a dictionary.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(corpus_file, \"r\") as file:\n",
    "            corpus_dict = json.load(file)\n",
    "        logger.info(f\"Corpus successfully loaded from {corpus_file}.\")\n",
    "        return corpus_dict\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load corpus from {corpus_file}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def expand_query_spreading_activation(query, max_depth=2, decay=0.6):\n",
    "    \"\"\"\n",
    "    Expand query using spreading activation from WordNet.\n",
    "    Args:\n",
    "        query (str): Original user query.\n",
    "        max_depth (int): How far to propagate in WordNet graph.\n",
    "        decay (float): Weight decay f actor at each propagation step.\n",
    "    Returns:\n",
    "        str: Expanded query string with semantically related words.\n",
    "    \"\"\"\n",
    "    query = preprocess_text(query)\n",
    "    tokens = query.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    activation = defaultdict(float) \n",
    "\n",
    "    for token in tokens:\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        activation[lemma] += 1.0  # base activation\n",
    "\n",
    "        frontier = [(lemma, 0, 1.0)]  # (word, depth, activation_level)\n",
    "        visited = set()\n",
    "\n",
    "        while frontier:\n",
    "            current_word, depth, current_activation = frontier.pop()\n",
    "            if depth >= max_depth or current_word in visited:\n",
    "                continue\n",
    "            visited.add(current_word)\n",
    "\n",
    "            synsets = wn.synsets(current_word)\n",
    "            related_words = set()\n",
    "\n",
    "            for syn in synsets:\n",
    "                related_words.update(lemma.name() for lemma in syn.lemmas())\n",
    "                if syn.hypernyms():\n",
    "                    for h in syn.hypernyms():\n",
    "                        related_words.update(lemma.name() for lemma in h.lemmas())\n",
    "\n",
    "            for related in related_words:\n",
    "                if related not in visited:\n",
    "                    activation[related] += current_activation * decay\n",
    "                    frontier.append((related, depth + 1, current_activation * decay))\n",
    "\n",
    "    # Generate expanded query\n",
    "    expanded_query = []\n",
    "    for word, weight in sorted(activation.items(), key=lambda x: -x[1]):\n",
    "        # Repeat words proportionally to their activation strength (rounded)\n",
    "        expanded_query.extend([word] * int(round(weight)))\n",
    "\n",
    "    return \" \".join(expanded_query)\n",
    "\n",
    "def generate_esa_vectors(text, corpus_file=\"topics/topics_top_1000_lem_wiki.json\"):\n",
    "    '''\n",
    "    Generate ESA vectors for the given text using the lemmatized corpus.\n",
    "    Args:\n",
    "        text (str): The input text to generate ESA vectors for.\n",
    "    Returns:\n",
    "        list: The ESA vectors for the input text.\n",
    "    '''\n",
    "    \n",
    "    logger.info(\"Generating ESA vectors.\")\n",
    "    \n",
    "    corpus = load_corpus(corpus_file)  # Load the corpus\n",
    "    if not corpus:\n",
    "        logger.error(\"Corpus is empty or could not be loaded.\")\n",
    "        return [], []\n",
    "\n",
    "    # preprocessing \n",
    "    expanded_query = expand_query_spreading_activation(text)\n",
    "    sentences = sent_tokenize(expanded_query)\n",
    "    processed_sentences = [preprocess_text(s) for s in sentences]\n",
    "    processed_corpus = list(corpus.values())\n",
    "    all_documents = processed_sentences + processed_corpus\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")          # Create a TF-IDF vectorizer\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_documents)      # Fit and transform the documents\n",
    "\n",
    "    esa_vectors = []\n",
    "    # Generate ESA vectors for each processed sentence\n",
    "    for i in range(len(processed_sentences)):\n",
    "        similarities = cosine_similarity(tfidf_matrix[i:i+1], tfidf_matrix[len(processed_sentences):])\n",
    "        esa_vector = similarities.flatten()\n",
    "        esa_vectors.append(esa_vector)\n",
    "    \n",
    "    if esa_vectors:         # ESA vectors are generated\n",
    "        esa_vectors = np.mean(esa_vectors, axis=0)\n",
    "        return esa_vectors.tolist()\n",
    "    else:\n",
    "        logger.error(\"No ESA vectors generated.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Main function to run the ESA vector generation.\n",
    "    '''\n",
    "    # Example text\n",
    "    text = \"what are the structural and aeroelastic problems associated with flight of high speed aircraft .\"\n",
    "    \n",
    "    # Generate ESA vectors for the example text\n",
    "    esa_vectors = generate_esa_vectors(text, corpus_file=\"topics/topics_top_100_lem_wiki.json\")\n",
    "    \n",
    "    if esa_vectors:\n",
    "        print(\"ESA Vectors generated successfully.\")\n",
    "        print(esa_vectors)\n",
    "    else:\n",
    "        print(\"Failed to generate ESA vectors.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fceb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "cranfield = []\n",
    "with open(\"cranfield/cran_docs.json\", \"r\") as f:\n",
    "    cranfield = json.load(f)\n",
    "    f.close()\n",
    "bodies = {data['id']:data['body'] for data in cranfield}\n",
    "    \n",
    "corpus_sizes = [5, 10, 50, 100, 500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a31f63f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m esa_dict = defaultdict()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, doc_body \u001b[38;5;129;01min\u001b[39;00m bodies.items():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     esa_dict[doc_id] = \u001b[43mgenerate_esa_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopics/topics_top_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcorpus_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_lem_wiki.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtopics/topics_top_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorpus_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_docs_sa_esa.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     11\u001b[39m     json.dump(esa_dict, f)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mgenerate_esa_vectors\u001b[39m\u001b[34m(text, corpus_file)\u001b[39m\n\u001b[32m    144\u001b[39m expanded_query = expand_query_spreading_activation(text)\n\u001b[32m    145\u001b[39m sentences = sent_tokenize(expanded_query)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m processed_sentences = [\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[32m    147\u001b[39m processed_corpus = \u001b[38;5;28mlist\u001b[39m(corpus.values())\n\u001b[32m    148\u001b[39m all_documents = processed_sentences + processed_corpus\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     40\u001b[39m tokens = word_tokenize(text.lower()) \n\u001b[32m     41\u001b[39m tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.isalnum()]\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m     44\u001b[39m lemmatizer = WordNetLemmatizer()\n\u001b[32m     45\u001b[39m tokens = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[39m, in \u001b[36mWordListCorpusReader.words\u001b[39m\u001b[34m(self, fileids, ignore_lines_startswith)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     20\u001b[39m         line\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.startswith(ignore_lines_startswith)\n\u001b[32m     23\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[39m, in \u001b[36mCorpusReader.raw\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m    216\u001b[39m contents = []\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    219\u001b[39m         contents.append(fp.read())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[39m, in \u001b[36mCorpusReader.open\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.encoding(file)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:326\u001b[39m, in \u001b[36mFileSystemPathPointer.open\u001b[39m\u001b[34m(self, encoding)\u001b[39m\n\u001b[32m    324\u001b[39m stream = \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m._path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     stream = \u001b[43mSeekableUnicodeStreamReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\compat.py:41\u001b[39m, in \u001b[36mpy3_data.<locals>._decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decorator\u001b[39m(*args, **kwargs):\n\u001b[32m     40\u001b[39m     args = (args[\u001b[32m0\u001b[39m], add_py3_data(args[\u001b[32m1\u001b[39m])) + args[\u001b[32m2\u001b[39m:]\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:1037\u001b[39m, in \u001b[36mSeekableUnicodeStreamReader.__init__\u001b[39m\u001b[34m(self, stream, encoding, errors)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28mself\u001b[39m._rewind_numchars = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The number of characters that have been returned since the\u001b[39;00m\n\u001b[32m   1033\u001b[39m \u001b[33;03m   read that started at ``_rewind_checkpoint``.  This is used,\u001b[39;00m\n\u001b[32m   1034\u001b[39m \u001b[33;03m   together with ``_rewind_checkpoint``, to backtrack to the\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m   beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m \u001b[38;5;28mself\u001b[39m._bom = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_bom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The length of the byte order marker at the beginning of\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[33;03m   the stream (or None for no byte order marker).\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:1410\u001b[39m, in \u001b[36mSeekableUnicodeStreamReader._check_bom\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1406\u001b[39m bom_info = \u001b[38;5;28mself\u001b[39m._BOM_TABLE.get(enc)\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bom_info:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;66;03m# Read a prefix, to check against the BOM(s)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m     \u001b[38;5;28mbytes\u001b[39m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28mself\u001b[39m.stream.seek(\u001b[32m0\u001b[39m)\n\u001b[32m   1413\u001b[39m     \u001b[38;5;66;03m# Check for each possible BOM.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for corpus_size in corpus_sizes:\n",
    "    \n",
    "    esa_dict = defaultdict()\n",
    "    \n",
    "    for doc_id, doc_body in bodies.items():\n",
    "        esa_dict[doc_id] = generate_esa_vectors(doc_body, corpus_file=f\"topics/topics_top_{corpus_size}_lem_wiki.json\")\n",
    "    \n",
    "    with open(f\"topics/topics_top_{corpus_size}_docs_sa_esa.json\", \"w\") as f:\n",
    "        json.dump(esa_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class ESAInformationRetrieval:\n",
    "    \n",
    "    def __init__(self, corpus_size=1000):\n",
    "        \n",
    "        self.corpus_size = corpus_size\n",
    "        self.docIDs = []\n",
    "        self.docVectors = None\n",
    "    \n",
    "    def build_vectors(self):\n",
    "        \n",
    "        with open(f\"topics/topics_top_{self.corpus_size}_docs_sa_esa.json\", \"r\") as f:\n",
    "            doc_esa_vectors = json.load(f)\n",
    "            f.close()\n",
    "        self.docIDs = [int(doc_id) for doc_id in doc_esa_vectors.keys()]\n",
    "        self.docVectors = np.array([x for x in list(doc_esa_vectors.values()) if len(x) != 0])\n",
    "    \n",
    "    def expand_query(self, query):\n",
    "        \"\"\"\n",
    "        Expand the query using spreading activation\n",
    "        \"\"\"\n",
    "        # Preprocess the original query\n",
    "        processed_query = preprocess_text(query)\n",
    "        \n",
    "        # Expand query using spreading activation\n",
    "        expanded_query = expand_query_spreading_activation(processed_query)\n",
    "        \n",
    "        # Generate ESA vector for expanded query\n",
    "        expanded_vector = generate_esa_vectors(expanded_query, corpus_file=f\"topics/topics_top_{self.corpus_size}_lem_wiki.json\")\n",
    "        \n",
    "        return expanded_vector\n",
    "    \n",
    "    def rank(self, query):\n",
    "        \n",
    "        # Get expanded query vector\n",
    "        queryVector = self.expand_query(query)\n",
    "                \n",
    "        sim_matrix = cosine_similarity(np.array(queryVector).reshape(1, -1), self.docVectors)\n",
    "              \n",
    "        for row in sim_matrix:\n",
    "            ranked_indices = np.argsort(-row)\n",
    "            ranked_docIDs = [self.docIDs[i] for i in ranked_indices]\n",
    "            \n",
    "        return ranked_docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bdf9f82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     18\u001b[39m query_ids, queries = [item[\u001b[33m\"\u001b[39m\u001b[33mquery number\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m queries_json], [item[\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m queries_json]\n\u001b[32m     21\u001b[39m evaluator = Evaluation()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m doc_IDs_ordered = [\u001b[43mir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[32m     25\u001b[39m precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m11\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mESAInformationRetrieval.rank\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrank\u001b[39m(\u001b[38;5;28mself\u001b[39m, query):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     queryVector = \u001b[43mgenerate_esa_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopics/topics_top_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_lem_wiki.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     sim_matrix = cosine_similarity(np.array(queryVector).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m), \u001b[38;5;28mself\u001b[39m.docVectors)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m sim_matrix:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mgenerate_esa_vectors\u001b[39m\u001b[34m(text, corpus_file)\u001b[39m\n\u001b[32m    144\u001b[39m expanded_query = expand_query_spreading_activation(text)\n\u001b[32m    145\u001b[39m sentences = sent_tokenize(expanded_query)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m processed_sentences = [\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[32m    147\u001b[39m processed_corpus = \u001b[38;5;28mlist\u001b[39m(corpus.values())\n\u001b[32m    148\u001b[39m all_documents = processed_sentences + processed_corpus\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     40\u001b[39m tokens = word_tokenize(text.lower()) \n\u001b[32m     41\u001b[39m tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.isalnum()]\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m     44\u001b[39m lemmatizer = WordNetLemmatizer()\n\u001b[32m     45\u001b[39m tokens = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[39m, in \u001b[36mWordListCorpusReader.words\u001b[39m\u001b[34m(self, fileids, ignore_lines_startswith)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     20\u001b[39m         line\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.startswith(ignore_lines_startswith)\n\u001b[32m     23\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[39m, in \u001b[36mCorpusReader.raw\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m    216\u001b[39m contents = []\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    219\u001b[39m         contents.append(fp.read())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[39m, in \u001b[36mCorpusReader.open\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.encoding(file)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m.open(encoding)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:334\u001b[39m, in \u001b[36mFileSystemPathPointer.join\u001b[39m\u001b[34m(self, fileid)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[32m    333\u001b[39m     _path = os.path.join(\u001b[38;5;28mself\u001b[39m._path, fileid)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\compat.py:41\u001b[39m, in \u001b[36mpy3_data.<locals>._decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decorator\u001b[39m(*args, **kwargs):\n\u001b[32m     40\u001b[39m     args = (args[\u001b[32m0\u001b[39m], add_py3_data(args[\u001b[32m1\u001b[39m])) + args[\u001b[32m2\u001b[39m:]\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shrey\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:311\u001b[39m, in \u001b[36mFileSystemPathPointer.__init__\u001b[39m\u001b[34m(self, _path)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[33;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[32m    306\u001b[39m \n\u001b[32m    307\u001b[39m \u001b[33;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m _path = os.path.abspath(_path)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % _path)\n\u001b[32m    313\u001b[39m \u001b[38;5;28mself\u001b[39m._path = _path\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from evaluation import Evaluation\n",
    "\n",
    "if not os.path.exists(\"spreading_activation_esa\"):\n",
    "\n",
    "    os.makedirs(\"spreading_activation_esa\")\n",
    "\n",
    "corpus_sizes = [5, 10, 50, 100, 500, 1000]\n",
    "\n",
    "for corpus_size in corpus_sizes:\n",
    "    ir = ESAInformationRetrieval(corpus_size=corpus_size)\n",
    "    ir.build_vectors()\n",
    "\n",
    "    queries_json = json.load(open(\"cranfield/cran_queries.json\", 'r'))\n",
    "    qrels = json.load(open(\"cranfield/cran_qrels.json\", 'r'))\n",
    "\n",
    "    query_ids, queries = [item[\"query number\"] for item in queries_json], [item[\"query\"] for item in queries_json]\n",
    "\n",
    "\n",
    "    evaluator = Evaluation()\n",
    "\n",
    "    doc_IDs_ordered = [ir.rank(query) for query in queries]\n",
    "\n",
    "    precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []\n",
    "\n",
    "    for k in range(1, 11):\n",
    "        precision = evaluator.meanPrecision(doc_IDs_ordered, query_ids, qrels, k)\n",
    "        precisions.append(precision)\n",
    "        recall = evaluator.meanRecall(doc_IDs_ordered, query_ids, qrels, k)\n",
    "        recalls.append(recall)\n",
    "        fscore = evaluator.meanFscore(doc_IDs_ordered, query_ids, qrels, k)\n",
    "        fscores.append(fscore)\n",
    "        #print(\"Precision, Recall and F-score @ \" + str(k) + \" : \" + str(precision) + \", \" + str(recall) + \", \" + str(fscore))\n",
    "        MAP = evaluator.meanAveragePrecision(doc_IDs_ordered, query_ids, qrels, k)\n",
    "        MAPs.append(MAP)\n",
    "        nDCG = evaluator.meanNDCG(doc_IDs_ordered, query_ids, qrels, k)\n",
    "        nDCGs.append(nDCG)\n",
    "        #print(\"MAP, nDCG @ \" + str(k) + \" : \" + str(MAP) + \", \" + str(nDCG))\n",
    "    \n",
    "    # Plot the metrics and save plot \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(range(1, 11), precisions, label=\"Precision\")\n",
    "    plt.plot(range(1, 11), recalls, label=\"Recall\")\n",
    "    plt.plot(range(1, 11), fscores, label=\"F-Score\")\n",
    "    plt.plot(range(1, 11), MAPs, label=\"MAP\")\n",
    "    plt.plot(range(1, 11), nDCGs, label=\"nDCG\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Evaluation Metrics - Corpus Size = {corpus_size}\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.savefig(f\"esa/eval_plot_{corpus_size}.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
